{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5_Task0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sosodoit/yolov5/blob/main/YOLOv5_Task0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMdkRcZN5nIO"
      },
      "source": [
        "<ol type=\"I\">\n",
        "  <li>Task 1 (사전작업)</li>\n",
        "  <dl>YOLOv5 모델작업에 들어가기전, 데이터를 확인하고 정제한 작업 내용을 담고 있습니다(<a href=\"https://colab.research.google.com/drive/1fBpA9nP9l9pPbSCTFzNk1J2amQNDTZbQ?usp=sharing\">link</a>)</dl>\n",
        "  <ul type=\"square\">\n",
        "  <li>Putting data into a folder</li>\n",
        "  <li>Convert the Annotations into the YOLOv5 Format</li>\n",
        "  <li>Testing the annotations</li>\n",
        "  <li>Partition the Dataset</li>\n",
        "  </ul>\n",
        "  <li>Task 2 (학습)</li>\n",
        "  <dl>YOLOv5 환경설정부터 학습한 작업내용을 담고 있습니다(<a href=\"https://colab.research.google.com/drive/1zHrTaAlsT4tDxZhGhUYK1ZIGjtrrvv-y?usp=sharing\">link</a>)</dl>\n",
        "  <ul type=\"square\">\n",
        "  <li>Set up the environment</li>\n",
        "  <li>Data Config File</li>\n",
        "  <li>Hyper-parameter Config File</li>\n",
        "  <li>Custom Network Architecture</li>\n",
        "  <li>Train the Model</li>\n",
        "  </ul>\n",
        "  <li>Task 3 (결과)</li>\n",
        "  <dl>학습 후 테스트와 성능평가의 결과내용을 담고 있습니다(<a href=\"https://colab.research.google.com/drive/1---iUi0KXpVVjn95BNmsDyScnzLmFY6p?usp=sharing\">link</a>)</dl>\n",
        "  <ul type=\"square\">\n",
        "  <li>Test</li>\n",
        "  <li>Computing the mAP on test dataset</li>\n",
        "  </ul>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1gvEmCWFLms"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwYtVwq_sGIp"
      },
      "source": [
        "<strong><center><h2>\n",
        "Task 0 (YOLOv5 note)</h2></strong>\n",
        "용어, 개념, 궁금한 문제들 기록</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0n1AHiIHCZm"
      },
      "source": [
        "# **분류와 검출의 차이점**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?id=1X5c7M623yWcVXObUUqGgnyNiDaf8WS5q\" width=\"800\" height=\"250\" />\n",
        "\n",
        "1. O. Recognition\n",
        "  - 이미지에 물체가 있다는 것을 인지하는 것. 물체의 존재 여부를 확인할 수 있지만 그 물체가 뭔지는 모름.\n",
        "2. Image Classification\n",
        "  - 이미지 전체 혹은 이미지 내 물체의 클래스를 분류하는 것(ex. 개와 고양이)\n",
        "3. O. Localization\n",
        "  - 이미지 내에 존재하는 단일 물체에 대한 위치 및 크기를 나타내는 것(Bounding Box)\n",
        "4. O. Detection\n",
        "  - 2와 3이 합쳐진 것. 이미지를 입력 값으로 넣으면 물체에 대한 바운딩 박스는 물론 박스마다 물체의 클래스를 분류할 수 있음.\n",
        "5. Image Segmentaion\n",
        "  - 4에서 발전된 것으로 이미지의 각 픽셀이 어느 클래스에 속하는지 예측하는 것."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05N2Gm4UyOoU"
      },
      "source": [
        "# **YOLO Model**\n",
        "\n",
        "||yolov5s|yolov5m|yolov5l|yolov5x|\n",
        "|------|---|---|---|---|\n",
        "|FP16|14 MB|41 MB|90 MB|168 MB|\n",
        "|V100|2.2 ms|2.9 ms|3.8 ms|6.0 ms|\n",
        "|coco|36.8 mAP|44.5 mAP|48.1 mAP|50.1 mAP|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Model|APval|APtest|AP50|SpeedGPU|FPSGPU|params|FLOPS|\n",
        "|------|---|---|---|---|---|---|---|\n",
        "|yolov5s|37.0|37.0|56.2|**2.4ms**|**416**|7.5M|13.2B|\n",
        "|yolov5m|44.3|44.3|63.2|3.4ms|294|21.8M|39.4B|\n",
        "|yolov5l|47.7|47.7|66.5|4.4ms|227|47.8M|88.1B|\n",
        "|yolov5x|**49.2**|**49.2**|**67.7**|6.9ms|145|89.0M|166.4B|\n",
        "|yolov5x+TTA|50.8|50.8|68.9|25.5ms|39|89.0M|354.3B|\n",
        "|yolov3-SPP|45.6|45.5|65.2|4.5ms|222|63.0M|118.0B|\n",
        "\n",
        "<br>\n",
        "\n",
        "1. yolov5x.yaml (가장 무거운 모델)\n",
        "성능이 제일 높지만 FPS는 가장 낮음\n",
        "2. yolov5l.yaml\n",
        "3. yolov5m.yaml\n",
        "4. yolov5s.yaml (가장 가벼운 모델)\n",
        "성능이 제일 낮지만 FPS가 가장 높음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqtQbifzm1o"
      },
      "source": [
        "## **YOLO 버전별 성능**\n",
        "<dl>yolov4는 MS COCO 데이터셋에서 AP : 43.5 % (AP50:65.7%), 65 FPS(Tesla V100 그래픽카드, 거의 실시간에 가까운 높은 FPS)를 달성했다.</dl>\n",
        "<img src=\"http://drive.google.com/uc?id=1X-ZClDL5QqW2cIji2FmFqsZ4Qb-kbkDg\" width=\"470\" height=\"288\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TprXVhTaOGcJ"
      },
      "source": [
        "## **YOLO 성능평가지표**\n",
        "\n",
        "<dl>\n",
        "AP가 높으면 높을수록 그 알고리즘의 성능이 전체적으로 우수하다는 의미이다. 컴퓨터 비전 분야에서 물체 인식 알고리즘의 성능은 대부분 AP로 평가한다.\n",
        "\n",
        "* [관련개념정리](https://drive.google.com/file/d/1XBLONkqmgSq5ENR0aiHs-o1cEYfRE1YQ/view?usp=sharing)\n",
        "\n",
        "</dl>\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?id=1XS3R2ZJQVlXL7S0Vdnq1fjqArT1Eo7iS\" width=\"470\" height=\"288\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgNWiZPPxVR5"
      },
      "source": [
        "## **AP50 (Pascal VOC mAP)**\n",
        "<dl>Pascal VOC는 IoU > 0.5 인 detection은 true, 그 이하는 false로 평가하는 방식을 사용</dl>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvEAYxJTyfxk"
      },
      "source": [
        "## **COCO**\n",
        "<dl>IoU > 0.5, IoU > 0.55, IoU > 0.6, ..., IoU > 0.95 각각을 기준으로 AP를 계산한 후 이들의 평균을 취하는 방식을 사용</dl>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RWpKxM0ybph"
      },
      "source": [
        "## **FPS**\n",
        "(Frame Per Second 초당 frame 수)\n",
        "\n",
        "영상에서 초당 프레임이 많을수록 움직임이 더 자연스러울 것이다. **O.D에서 FPS라는 건 초당 detection(탐지)하는 비율을 의미한다.** 초당 30개의 프레임에 대해 detection을 수행하면 30fps가 된다. **우리가 real time이라고 느끼는 FPS는 30fps로 초당 프레임을 30개 이상 처리하면 자연스러운 영상으로 인식한다**(30fps=움직임이 부드러움, 60fps=움직임이 더 부드러움).\n",
        "\n",
        "* **O.D의 성능측정을 할 때는 accuracy도 중요하지만 speed도 중요하다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Qy7v-lEsfi"
      },
      "source": [
        "---\n",
        "# **YOLOv5 ?**\n",
        "(You Only Look Once) 딥러닝 기법을 활용한 매우 빠른 속도로 이미지상에 존재하는 물체의 종류와 2D 좌표를 획득하여 **학습한 물체의 종류와 위치를 실시간으로 파악할 수 있는** 'Real time object detection model'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWfZx9zBhxqf"
      },
      "source": [
        "# **How to choice batch size**\n",
        "\n",
        "- batch size와 성능의 상관 관계는 아직 정확하게 규정되지는 않았다. 데이터에 따라 그 기준이 달라지기 때문​에. 다만, **일반적으로 32, 64 크기의 mini-batch가 성능에는 가장 좋다고 알려져 있다**고한다.\n",
        "\n",
        "  1. **batch size를 줄임으로써 얻는 장점**\n",
        "    - 필요한 메모리 감소: 전체 데이터를 쪼개어 여러 번 학습하는 것이기 때문에 최소 요구 메모리량을 줄일 수 있음.\n",
        "\n",
        "  2. **batch size를 늘임으로써 얻는 장점**\n",
        "    - 아래 graph를 보면 전체 데이터를 활용한 Batch의 경우(파란색 그래프)보다 batch size가 작은 Mni-batch의 경우(초록색 그래프)가 더 fluctuate 한 것을 확인할 수 있음. (더 flucatuate 하다는 것은 학습이 불안정 해진다는 의미)\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?id=1X-KsjZnawqjr-xdeHVIKncg19uoyDTYs\" width=\"400\" height=\"250\" />\n",
        "\n",
        "  - 정리: **가용 메모리가 적을 때는 batch size를 상대적으로 작게, 보다 안정적으로 학습을 시키고 싶다면 batch size를 상대적으로 높게 설정해주면 됩니다.** 다만, 아래 [논문 정리글](https://blog.lunit.io/2018/08/03/batch-size-in-deep-learning/)에서도 확인할 수 있듯이 **batch size가 커질 수록 일반화 성능은 감소하는 경우가 다소 확인이 되었으니 그 점만 유의**해주시면 되겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ROpT1uh5yBd"
      },
      "source": [
        "# **How to increase performance**\n",
        "\n",
        "- 데이터 셋이 충분히 크고 정확한 라벨링이 되어있는가\n",
        "\n",
        "- **background image 넣기** (백그라운드 이미지는 탐지할 물체가 없는 데이터들을 의미한다. 이 백그라운드 이미지를 넣어주면 False Positives(FP)가 줄어드는 효과를 볼 수 있다. 전체 학습 데이터 셋에 0-10% 정도 넣는 것을 추천한다고 한다.)\n",
        "\n",
        "- Pretrained weights 사용하기 (데이터 셋이 작을 경우 추천한다.)\n",
        "\n",
        "- **epoch**는 300부터 시작해서 overfit이 발생하면 줄이고 발생하지 않으면 600 1200 등으로 점점 늘리면 된다.\n",
        "\n",
        "- **이미지 사이즈**를 보면 coco는 640으로 학습한다. 작은 크기의 물체가 많을수록 높거나 원래의 해상도 사이즈를 쓰면 좋다. 최고의 inference 성능을 내려면 inference 돌릴 때 input으로 들어가는 이미지 사이즈가 학습 때 설정한 사이즈와 같아야 한다.\n",
        "- 기본값으로 저장된 모델 하이퍼 파라미터들은 data폴더 안에 hyp.scratch.yaml에서 찾을 수 있다. 먼저 이 하이퍼파리미터로 학습하는 것을 추천한다.\n",
        "\n",
        "  - 인자값으로 --evolve를 줘서 Hyperparameter evolution을 사용할 수 있다고 한다. 시나리오 50번을 돌려서 하이퍼파라미터를 최적화하고 결과값은 evlove.txt에 저장된다고 한다.\n",
        "  \n",
        "  - Hyperparameter evolution in yolov5 : Genetic Algorithm(GA) 사용해서 하이퍼 파라미터를 최적화하는 방법이라고 한다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxRaT77Y4InS"
      },
      "source": [
        "# **train.py 인자값 설명**\n",
        "\n",
        "|||\n",
        "|----|----|\n",
        "|--img : 이미지의 크기. 이미지는 정사각형이다. 종횡비를 유지하면서 원본 이미지의 크기가 조정되고 이미지의 긴 쪽의 숫자로 크기가 조정된다. 짧은 쪽은 회색으로 패딩된다.|\n",
        "|--batch : 전체 데이터셋 중 한 번에 몇 개까지 학습시킬 것인지|\n",
        "|--epochs : 학습시키는 데이터셋을 몇 번 반복시켜 학습시킬 것인지|\n",
        "|--data : 데이터셋(이미지 경로, 레이블)에 대한 정보가 포함된 데이터 YAML 파일|\n",
        "|--cfg : 모델 아키텍처. yolo5s.yaml, yolov5m.yaml, yolov5l.yaml, yolov5x.yaml의 4가지 선택 항목이 있다. 이러한 모델의 크기와 복잡성은 오름차순으로 증가하며 개체 감지 작업의 복잡성에 맞는 모델을 선택할 수 있다. 사용자 지정 아키텍처로 작업하려는 경우 네트워크 아키텍처를 지정하는 모델 폴더에 YAML 파일을 정의해야 한다.|\n",
        "|--weights : 훈련을 시작하려는 사전 훈련된 가중치. 처음부터 훈련하려면 --weights ' '를 사용.|\n",
        "|--name : (폴더명지정)해당 경로에 결과가 저장됨|\n",
        "|--workers : Number of CPU workers|\n",
        "|--hyp : 초매개변수 선택을 설명하는 YAML파일. 초매개변수를 정의하는 방법의 예는 data/hyp.scratch.yaml을 참조. 지정하지 않으면 data/hyp.scratch.yaml 파일이 사용된다.|\n",
        "|--resume : 학습이 중간에 멈추면 **!python train.py --resume** 코드를 통해 제일 최근에 저장된 weight \"last.py\"부터 학습을 다시 시작할 수 있다(epoch도 멈췄던 지점부터 다시 학습이 된다)(특정 weight부터 학습을 다시 하고 싶다면 **!python train.py --resume runs/train/weights/last.pt** 해당 weight 경로를 적어주면 된다).|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9O_8Z9_b8aq"
      },
      "source": [
        "# **batch/epoch/iteration**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?id=1WnTX6_1CgOwEaq_31BF1a8zPLLkpoJsT\" width=\"460\" height=\"280\" />\n",
        "\n",
        "- **epoch 1** : 전체 데이터 셋에 대해 한 번 학습을 완료한 상태\n",
        "- **epoch 40** : 전체 데이터 셋을 40번 사용해서 학습을 완료한 상태\n",
        "  - 신경망에서 사용되는 역전파 알고리즘(backpropagation algorithm)은 파라미터를 사용하여 입력부터 출력까지의 각 계층의 weight를 계산하는 과정을 거치는 순방향 패스(forward pass), forward pass를 반대로 거슬러 올라가며 다시 한 번 계산 과정을 거처 기존의 weight를 수정하는 역방향 패스(backward pass)로 나뉜다. 즉, **전체 데이터 셋에 대해 해당 과정(forward pass + backward pass)이 완료되면 한 번의 epoch가 진행됐다고 볼 수 있다.**\n",
        "  - 우리는 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있다. epoch 값이 너무 작다면 underfitting. 너무 크다면 overfitting이 발생할 확률이 높다.\n",
        "\n",
        "- **batch size** : 한 번의 batch마다 주는 데이터 샘플사이즈\n",
        "- **iteration** : epoch를 나누어서 실행하는 횟수\n"
      ]
    }
  ]
}